{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/workspaces/Face-Embeddings\")\n",
    "import random\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "\n",
    "from src.data.dataset import TripletDataset\n",
    "from src.data.transform import get_train_transforms\n",
    "\n",
    "\n",
    "def tensor2array(img: Tensor):\n",
    "    img = img.numpy().transpose(1, 2, 0)\n",
    "    img = (img - img.min()) / (img.max() - img.min())\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize_triplet(anchor, positive, negative, figsize=(15, 5)):\n",
    "    \"\"\"\n",
    "    Visualize a triplet (anchor, positive, negative) in a grid.\n",
    "\n",
    "    Args:\n",
    "        anchor: PIL Image of the anchor\n",
    "        positive: PIL Image of the positive sample\n",
    "        negative: PIL Image of the negative sample\n",
    "        figsize: Size of the figure (width, height)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "\n",
    "    # Display images\n",
    "    axes[0].imshow(anchor)\n",
    "    axes[0].set_title(\"Anchor\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(positive)\n",
    "    axes[1].set_title(\"Positive (Same Identity)\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    axes[2].imshow(negative)\n",
    "    axes[2].set_title(\"Negative (Different Identity)\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TripletDataset(\n",
    "    data_dir=Path(\"../data/processed/train\"),\n",
    "    transforms=get_train_transforms(224, 224, use_face_swap=False),\n",
    "    triplet_cache_size=10000,  # Опционально\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = random.randint(0, 100)\n",
    "anchor = Image.fromarray(tensor2array(dataset[n][\"anchor\"]))\n",
    "positive = Image.fromarray(tensor2array(dataset[n][\"positive\"]))\n",
    "negative = Image.fromarray(tensor2array(dataset[n][\"negative\"]))\n",
    "# Visualize the triplet\n",
    "fig = visualize_triplet(anchor, positive, negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Загрузка метаданных\n",
    "with open(Path(\"data/original/train/meta.json\"), \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "# Создание директорий для разделенных данных\n",
    "base_dir = Path(\"data/processed\")\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    (base_dir / split / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Получение всех уникальных идентификаторов лиц\n",
    "face_ids = set()\n",
    "for path in meta.keys():\n",
    "    face_id = path.split(\"/\")[0]\n",
    "    face_ids.add(face_id)\n",
    "\n",
    "# Преобразование в список для перемешивания\n",
    "face_ids = list(face_ids)\n",
    "random.shuffle(face_ids)\n",
    "\n",
    "# Разделение на train (70%), val (15%), test (15%)\n",
    "total = len(face_ids)\n",
    "train_ids = set(face_ids[: int(0.7 * total)])\n",
    "val_ids = set(face_ids[int(0.7 * total) : int(0.85 * total)])\n",
    "test_ids = set(face_ids[int(0.85 * total) :])\n",
    "\n",
    "# Создание отдельных meta.json для каждого сплита\n",
    "train_meta = {}\n",
    "val_meta = {}\n",
    "test_meta = {}\n",
    "\n",
    "# Копирование файлов в соответствующие директории\n",
    "src_dir = Path(\"data/original/train\")\n",
    "\n",
    "for path, is_fake in tqdm(meta.items()):\n",
    "    face_id = path.split(\"/\")[0]\n",
    "\n",
    "    # Определение целевого сплита и соответствующего meta-словаря\n",
    "    if face_id in train_ids:\n",
    "        split = \"train\"\n",
    "        split_meta = train_meta\n",
    "    elif face_id in val_ids:\n",
    "        split = \"val\"\n",
    "        split_meta = val_meta\n",
    "    else:\n",
    "        split = \"test\"\n",
    "        split_meta = test_meta\n",
    "\n",
    "    # Добавление записи в соответствующий meta.json\n",
    "    split_meta[path] = is_fake\n",
    "\n",
    "    # Создание директории для лица, если её нет\n",
    "    face_dir = base_dir / split / \"images\" / face_id\n",
    "    face_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Исходный и целевой пути\n",
    "    src_path = src_dir / \"images\" / path\n",
    "    dst_path = base_dir / split / \"images\" / path\n",
    "\n",
    "    # Копирование файла\n",
    "    shutil.copy(src_path, dst_path)\n",
    "\n",
    "# Сохранение meta.json для каждого сплита\n",
    "for split, split_meta in [\n",
    "    (\"train\", train_meta),\n",
    "    (\"val\", val_meta),\n",
    "    (\"test\", test_meta),\n",
    "]:\n",
    "    with open(base_dir / split / \"meta.json\", \"w\") as f:\n",
    "        json.dump(split_meta, f, indent=4)\n",
    "\n",
    "print(\n",
    "    f\"Разделение завершено. Обучающая выборка: {len(train_ids)} лиц, \"\n",
    "    f\"Валидационная выборка: {len(val_ids)} лиц, \"\n",
    "    f\"Тестовая выборка: {len(test_ids)} лиц\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Путь к обработанным данным\n",
    "processed_dir = Path(\"data/processed\")\n",
    "\n",
    "\n",
    "# Функция для анализа сплита\n",
    "def analyze_split(split_name):\n",
    "    split_dir = processed_dir / split_name\n",
    "    meta_path = split_dir / \"meta.json\"\n",
    "\n",
    "    # Загрузка метаданных\n",
    "    with open(meta_path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    # Подсчет количества изображений\n",
    "    total_images = len(meta)\n",
    "\n",
    "    # Подсчет количества реальных и поддельных изображений\n",
    "    fake_count = sum(1 for label in meta.values() if label == 1)\n",
    "    real_count = total_images - fake_count\n",
    "\n",
    "    # Подсчет количества уникальных лиц\n",
    "    face_ids = set(path.split(\"/\")[0] for path in meta.keys())\n",
    "    total_faces = len(face_ids)\n",
    "\n",
    "    # Подсчет изображений для каждого лица\n",
    "    face_image_counts = Counter(path.split(\"/\")[0] for path in meta.keys())\n",
    "\n",
    "    # Подсчет реальных и поддельных изображений для каждого лица\n",
    "    face_label_counts = {}\n",
    "    for path, is_fake in meta.items():\n",
    "        face_id = path.split(\"/\")[0]\n",
    "        if face_id not in face_label_counts:\n",
    "            face_label_counts[face_id] = {\"real\": 0, \"fake\": 0}\n",
    "\n",
    "        if is_fake == 1:\n",
    "            face_label_counts[face_id][\"fake\"] += 1\n",
    "        else:\n",
    "            face_label_counts[face_id][\"real\"] += 1\n",
    "\n",
    "    return {\n",
    "        \"split\": split_name,\n",
    "        \"total_images\": total_images,\n",
    "        \"real_images\": real_count,\n",
    "        \"fake_images\": fake_count,\n",
    "        \"total_faces\": total_faces,\n",
    "        \"face_image_counts\": face_image_counts,\n",
    "        \"face_label_counts\": face_label_counts,\n",
    "    }\n",
    "\n",
    "\n",
    "# Анализ всех сплитов\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "stats = {}\n",
    "\n",
    "for split in splits:\n",
    "    stats[split] = analyze_split(split)\n",
    "\n",
    "# Вывод основной статистики\n",
    "summary = []\n",
    "for split in splits:\n",
    "    s = stats[split]\n",
    "    summary.append(\n",
    "        {\n",
    "            \"Split\": split,\n",
    "            \"Total Images\": s[\"total_images\"],\n",
    "            \"Real Images\": s[\"real_images\"],\n",
    "            \"Fake Images\": s[\"fake_images\"],\n",
    "            \"Real %\": round(s[\"real_images\"] / s[\"total_images\"] * 100, 2),\n",
    "            \"Fake %\": round(s[\"fake_images\"] / s[\"total_images\"] * 100, 2),\n",
    "            \"Total Faces\": s[\"total_faces\"],\n",
    "            \"Avg Images per Face\": round(s[\"total_images\"] / s[\"total_faces\"], 2),\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Создание DataFrame для удобного отображения\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(\"Общая статистика по выборкам:\")\n",
    "print(summary_df)\n",
    "\n",
    "# Визуализация распределения классов\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x=\"Split\",\n",
    "    y=\"Total Images\",\n",
    "    hue=\"Class\",\n",
    "    data=pd.DataFrame(\n",
    "        [\n",
    "            {\"Split\": s[\"Split\"], \"Total Images\": s[\"Real Images\"], \"Class\": \"Real\"}\n",
    "            for s in summary\n",
    "        ]\n",
    "        + [\n",
    "            {\"Split\": s[\"Split\"], \"Total Images\": s[\"Fake Images\"], \"Class\": \"Fake\"}\n",
    "            for s in summary\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "plt.title(\"Распределение классов по выборкам\")\n",
    "plt.savefig(processed_dir / \"class_distribution.png\")\n",
    "\n",
    "# Анализ распределения количества изображений на лицо\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, split in enumerate(splits):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    counts = list(stats[split][\"face_image_counts\"].values())\n",
    "    sns.histplot(counts, kde=True)\n",
    "    plt.title(f\"{split.capitalize()}: Изображений на лицо\")\n",
    "    plt.xlabel(\"Количество изображений\")\n",
    "    plt.ylabel(\"Количество лиц\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(processed_dir / \"images_per_face.png\")\n",
    "\n",
    "print(\"\\nСтатистика сохранена в виде графиков в директории processed\")\n",
    "\n",
    "# Проверка физического наличия файлов\n",
    "for split in splits:\n",
    "    split_dir = processed_dir / split\n",
    "    meta_path = split_dir / \"meta.json\"\n",
    "\n",
    "    with open(meta_path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    missing_files = []\n",
    "    for path in meta.keys():\n",
    "        file_path = split_dir / \"images\" / path\n",
    "        if not file_path.exists():\n",
    "            missing_files.append(str(file_path))\n",
    "\n",
    "    if missing_files:\n",
    "        print(f\"\\nВ выборке {split} отсутствуют следующие файлы:\")\n",
    "        for path in missing_files[:10]:  # Показываем только первые 10\n",
    "            print(f\"  - {path}\")\n",
    "        if len(missing_files) > 10:\n",
    "            print(f\"  ... и еще {len(missing_files) - 10} файлов\")\n",
    "    else:\n",
    "        print(f\"\\nВсе файлы в выборке {split} присутствуют физически\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
